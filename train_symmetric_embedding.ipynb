{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "680a2ed7",
   "metadata": {},
   "source": [
    "# General Function Embedding (PyTorch)\n",
    "\n",
    "This notebook is now generalized to learn any target function f: R^{in_dim} -> R^{out_dim} using a compact bottleneck (latent) representation.\n",
    "\n",
    "What you can customize quickly:\n",
    "- Input size (in_dim), output size (out_dim), and latent size (embed_dim)\n",
    "- Target function via a simple Python callable\n",
    "- Model depth/width and activations\n",
    "- Sampling domain and dataset sizes\n",
    "\n",
    "Artifacts saved per run:\n",
    "- model.pt – trained weights + config metadata\n",
    "- embeddings_probe.npz – random probe inputs with embeddings and predictions\n",
    "\n",
    "Tip: Edit the `target_fn` in the setup cell to learn any function you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e06efc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.8.0\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# Setup & Config\n",
    "# --------------------\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from lib import (\n",
    "    BottleneckMLP,\n",
    "    OnlineFunctionDataset,\n",
    "    make_fixed_val_dataset,\n",
    "    train_model,\n",
    "    evaluate,\n",
    "    get_device,\n",
    "    set_seed,\n",
    "    next_run_path,\n",
    ")\n",
    "\n",
    "print(\"Torch:\", torch.__version__)\n",
    "device = get_device()\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Create a unique run folder (keeps your old results intact)\n",
    "_run_path = next_run_path(base_dir='runs', prefix='symmetry_run_')\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Data/function\n",
    "    in_dim: int = 2\n",
    "    out_dim: int = 1\n",
    "    embed_dim: int = 4\n",
    "    data_range: float = 3.0   # sample x in [-R, R]^in_dim\n",
    "\n",
    "    # Model\n",
    "    hidden: int = 64\n",
    "    enc_layers: int = 2\n",
    "    dec_layers: int = 2\n",
    "\n",
    "    # Training\n",
    "    seed: int = 42\n",
    "    n_train: int = 50_000\n",
    "    n_val: int = 5_000\n",
    "    batch_size: int = 256\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    epochs: int = 25\n",
    "\n",
    "    # IO\n",
    "    device: torch.device = device\n",
    "    model_path: str = os.path.join(_run_path, \"model.pt\")\n",
    "    probe_path: str = os.path.join(_run_path, \"embeddings_probe.npz\")\n",
    "\n",
    "cfg = Config()\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "# Define your target function here. It must map [N, in_dim] -> [N, out_dim].\n",
    "# Example: f(x) = sin(||x||) returning a single output regardless of in_dim.\n",
    "def target_fn(x: torch.Tensor) -> torch.Tensor:\n",
    "    r = torch.linalg.norm(x, dim=1, keepdim=True)\n",
    "    return torch.sin(r)  # shape [N,1]\n",
    "\n",
    "writer = SummaryWriter(log_dir=_run_path)\n",
    "writer.add_text(\n",
    "    'config',\n",
    "    str(\n",
    "        dict(\n",
    "            in_dim=cfg.in_dim,\n",
    "            out_dim=cfg.out_dim,\n",
    "            embed_dim=cfg.embed_dim,\n",
    "            hidden=cfg.hidden,\n",
    "            enc_layers=cfg.enc_layers,\n",
    "            dec_layers=cfg.dec_layers,\n",
    "            data_range=cfg.data_range,\n",
    "            seed=cfg.seed,\n",
    "        )\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d53536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: online sampling, 50000 samples/epoch; Validation: 5000 samples\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# Data: generic function via online sampling\n",
    "# --------------------\n",
    "train_dataset = OnlineFunctionDataset(\n",
    "    n=cfg.n_train,\n",
    "    in_dim=cfg.in_dim,\n",
    "    target_fn=target_fn,\n",
    "    data_range=cfg.data_range,\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "val_dataset = make_fixed_val_dataset(\n",
    "    n_val=cfg.n_val,\n",
    "    in_dim=cfg.in_dim,\n",
    "    target_fn=target_fn,\n",
    "    data_range=cfg.data_range,\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "print(f\"Training: online sampling, {cfg.n_train} samples/epoch; Validation: {cfg.n_val} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b0cc90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BottleneckMLP(\n",
       "  (enc): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=64, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): GELU(approximate='none')\n",
       "  )\n",
       "  (embed): Linear(in_features=64, out_features=4, bias=True)\n",
       "  (dec): Sequential(\n",
       "    (0): GELU(approximate='none')\n",
       "    (1): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (4): GELU(approximate='none')\n",
       "    (5): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------\n",
    "# Model: Encoder -> Embedding -> Decoder (generic)\n",
    "# --------------------\n",
    "model = BottleneckMLP(\n",
    "    in_dim=cfg.in_dim,\n",
    "    out_dim=cfg.out_dim,\n",
    "    embed_dim=cfg.embed_dim,\n",
    "    hidden=cfg.hidden,\n",
    "    enc_layers=cfg.enc_layers,\n",
    "    dec_layers=cfg.dec_layers,\n",
    ").to(cfg.device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24dbe778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/25 | val MSE: 0.003852\n",
      "Epoch 05/25 | val MSE: 0.000142\n",
      "Epoch 05/25 | val MSE: 0.000142\n",
      "Epoch 10/25 | val MSE: 0.000049\n",
      "Epoch 10/25 | val MSE: 0.000049\n",
      "Epoch 15/25 | val MSE: 0.000021\n",
      "Epoch 15/25 | val MSE: 0.000021\n",
      "Epoch 20/25 | val MSE: 0.000021\n",
      "Epoch 20/25 | val MSE: 0.000021\n",
      "Epoch 25/25 | val MSE: 0.000051\n",
      "Best val MSE: 0.000016\n",
      "Epoch 25/25 | val MSE: 0.000051\n",
      "Best val MSE: 0.000016\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# Training\n",
    "# --------------------\n",
    "result = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=cfg.epochs,\n",
    "    lr=cfg.lr,\n",
    "    weight_decay=cfg.weight_decay,\n",
    "    writer=writer,\n",
    "    device=cfg.device,\n",
    "    log_hist_every=100,\n",
    ")\n",
    "\n",
    "print(f\"Best val MSE: {result['best_val']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b75d0738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: /Users/noah-everett/Documents/Research/Embedding-Analysis/runs/symmetry_run_2/model.pt\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# Save model + handy metadata\n",
    "# --------------------\n",
    "payload = {\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"config\": {\n",
    "        \"in_dim\": cfg.in_dim,\n",
    "        \"out_dim\": cfg.out_dim,\n",
    "        \"hidden\": cfg.hidden,\n",
    "        \"embed_dim\": cfg.embed_dim,\n",
    "        \"enc_layers\": cfg.enc_layers,\n",
    "        \"dec_layers\": cfg.dec_layers,\n",
    "        \"data_range\": cfg.data_range,\n",
    "        \"seed\": cfg.seed,\n",
    "        \"target_fn\": getattr(target_fn, \"__name__\", str(target_fn)),\n",
    "    },\n",
    "    \"model_class\": \"BottleneckMLP\",\n",
    "}\n",
    "torch.save(payload, cfg.model_path)\n",
    "print(f\"Saved model to: {os.path.abspath(cfg.model_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38dc4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved probe embeddings to: /Users/noah-everett/Documents/Research/Embedding-Analysis/runs/symmetry_run_2/embeddings_probe.npz\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# Save probe embeddings for downstream analysis (works for any in/out/embed sizes)\n",
    "# --------------------\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Random probe inputs in the training domain\n",
    "    N_probe = 10240\n",
    "    X = np.random.uniform(-cfg.data_range, cfg.data_range, size=(N_probe, cfg.in_dim)).astype(np.float32)\n",
    "\n",
    "    xb = torch.from_numpy(X).to(cfg.device)\n",
    "    pred, z = model(xb)\n",
    "    pred = pred.cpu().numpy()\n",
    "    Z = z.cpu().numpy()\n",
    "\n",
    "    # Optionally compute ground truth if target_fn is defined here\n",
    "    try:\n",
    "        y_true = target_fn(xb).cpu().numpy()\n",
    "    except Exception:\n",
    "        y_true = None\n",
    "\n",
    "    try:\n",
    "        # Log embeddings to TensorBoard projector (works for any dimensionality)\n",
    "        writer.add_embedding(torch.from_numpy(Z), global_step=0, tag='embeddings/probe')\n",
    "        if cfg.embed_dim == 2:\n",
    "            import matplotlib.pyplot as plt\n",
    "            r = np.linalg.norm(X, axis=1)\n",
    "            fig, ax = plt.subplots(figsize=(6, 6))\n",
    "            sc = ax.scatter(Z[:, 0], Z[:, 1], c=r, s=5, cmap='viridis')\n",
    "            ax.set_title('Embeddings (colored by ||x||)')\n",
    "            ax.set_xlabel('z0')\n",
    "            ax.set_ylabel('z1')\n",
    "            plt.colorbar(sc, ax=ax, label='||x||')\n",
    "            writer.add_figure('embeddings/2d_scatter', fig)\n",
    "            plt.close(fig)\n",
    "    except Exception as e:\n",
    "        print('Warning: failed to log embeddings to TensorBoard:', e)\n",
    "\n",
    "# Save everything needed for downstream math/plots (general)\n",
    "if y_true is None:\n",
    "    np.savez_compressed(\n",
    "        cfg.probe_path,\n",
    "        x=X,                # [N,in_dim] inputs in input space\n",
    "        z=Z,                # [N,embed_dim] embeddings\n",
    "        pred=pred           # [N,out_dim] predicted f(x)\n",
    "    )\n",
    "else:\n",
    "    np.savez_compressed(\n",
    "        cfg.probe_path,\n",
    "        x=X,\n",
    "        z=Z,\n",
    "        pred=pred,\n",
    "        y=y_true\n",
    "    )\n",
    "print(f\"Saved probe embeddings to: {os.path.abspath(cfg.probe_path)}\")\n",
    "\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_3_11_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
